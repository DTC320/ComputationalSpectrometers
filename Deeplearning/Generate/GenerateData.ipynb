{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5661, 400)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"/Users/dongtianchi/Documents/GIT/SpectralReconstruction/ComputationalSpectrometers/Deeplearning/SpectrumData.npy\",allow_pickle=True)\n",
    "data = data.T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(data, dtype=np.float32)\n",
    "# 将数据集划分为训练集和验证集\n",
    "train_data = data[:5000]\n",
    "val_data = data[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建PyTorch数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(train_data)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(val_data)), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1,H2, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2,H2)\n",
    "        self.enc_mu = torch.nn.Linear(H2, latent_size)\n",
    "        self.enc_log_sigma = torch.nn.Linear(H2, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x)) \n",
    "        mu = self.enc_mu(x)\n",
    "        log_sigma = self.enc_log_sigma(x)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1,H2, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H2)\n",
    "        self.linear2 = torch.nn.Linear(H2, H1)\n",
    "        self.linear3 = torch.nn.Linear(H1, D_out)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mu = torch.tanh(self.linear3(x))  # 新增加的激活函数\n",
    "        return torch.distributions.Normal(mu, torch.ones_like(mu))\n",
    "    \n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, state):\n",
    "        q_z = self.encoder(state)\n",
    "        z = q_z.rsample()\n",
    "        return self.decoder(z), q_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE参数\n",
    "input_dim = 400\n",
    "hidden_dim1 = 256\n",
    "hidden_dim2 = 128\n",
    "latent_dim = 50\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# 设备和优化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 57954.71081542969 -367.8749694824219 0.09397052228450775\n",
      "Epoch [1/50], Train Loss: 369.1383, Val Loss: 368.5926\n",
      "1 57964.561950683594 -367.8750305175781 0.05442120507359505\n",
      "Epoch [2/50], Train Loss: 369.2010, Val Loss: 368.3226\n",
      "2 57900.02145385742 -368.1928405761719 0.04321391507983208\n",
      "Epoch [3/50], Train Loss: 368.7899, Val Loss: 368.4471\n",
      "3 57907.38638305664 -367.9693908691406 0.018992863595485687\n",
      "Epoch [4/50], Train Loss: 368.8369, Val Loss: 368.5718\n",
      "4 57887.64306640625 -368.2444763183594 0.02525072917342186\n",
      "Epoch [5/50], Train Loss: 368.7111, Val Loss: 368.5999\n",
      "5 57899.48489379883 -368.0853271484375 0.03785347193479538\n",
      "Epoch [6/50], Train Loss: 368.7865, Val Loss: 368.4368\n",
      "6 57884.781799316406 -367.9739990234375 0.08190701901912689\n",
      "Epoch [7/50], Train Loss: 368.6929, Val Loss: 368.2855\n",
      "7 57880.866455078125 -368.1640625 0.01809319108724594\n",
      "Epoch [8/50], Train Loss: 368.6679, Val Loss: 368.5770\n",
      "8 57885.64407348633 -367.9927673339844 0.1830027550458908\n",
      "Epoch [9/50], Train Loss: 368.6984, Val Loss: 368.7417\n",
      "9 57868.415283203125 -368.1653747558594 0.03420884907245636\n",
      "Epoch [10/50], Train Loss: 368.5886, Val Loss: 368.6112\n",
      "10 57873.482666015625 -368.0845947265625 0.016307557001709938\n",
      "Epoch [11/50], Train Loss: 368.6209, Val Loss: 368.4589\n",
      "11 57895.950439453125 -368.03802490234375 0.018268216401338577\n",
      "Epoch [12/50], Train Loss: 368.7640, Val Loss: 368.3308\n",
      "12 57881.05334472656 -368.103271484375 0.013670246116816998\n",
      "Epoch [13/50], Train Loss: 368.6691, Val Loss: 368.2830\n",
      "13 57882.08044433594 -368.5218811035156 0.03990554064512253\n",
      "Epoch [14/50], Train Loss: 368.6757, Val Loss: 368.2164\n",
      "14 57875.20608520508 -367.88726806640625 0.037453532218933105\n",
      "Epoch [15/50], Train Loss: 368.6319, Val Loss: 368.2398\n",
      "15 57886.58517456055 -368.1123962402344 0.018726786598563194\n",
      "Epoch [16/50], Train Loss: 368.7044, Val Loss: 368.2732\n",
      "16 57871.03726196289 -368.0380859375 0.004089194815605879\n",
      "Epoch [17/50], Train Loss: 368.6053, Val Loss: 368.5937\n",
      "17 57880.04946899414 -368.3431091308594 0.0066405669786036015\n",
      "Epoch [18/50], Train Loss: 368.6627, Val Loss: 368.5443\n",
      "18 57880.39709472656 -368.0567321777344 0.029252124950289726\n",
      "Epoch [19/50], Train Loss: 368.6649, Val Loss: 368.2706\n",
      "19 57880.998931884766 -368.04296875 0.026270577684044838\n",
      "Epoch [20/50], Train Loss: 368.6688, Val Loss: 368.5492\n",
      "20 57865.64028930664 -368.1611328125 0.06296168267726898\n",
      "Epoch [21/50], Train Loss: 368.5710, Val Loss: 368.8994\n",
      "21 57892.81802368164 -368.12103271484375 0.0008409270667470992\n",
      "Epoch [22/50], Train Loss: 368.7441, Val Loss: 368.5660\n",
      "22 57891.48538208008 -367.92626953125 0.03678605332970619\n",
      "Epoch [23/50], Train Loss: 368.7356, Val Loss: 368.3603\n",
      "23 57880.33563232422 -368.0138244628906 0.011551422998309135\n",
      "Epoch [24/50], Train Loss: 368.6646, Val Loss: 368.3349\n",
      "24 57875.54104614258 -368.08978271484375 0.02117719128727913\n",
      "Epoch [25/50], Train Loss: 368.6340, Val Loss: 368.3275\n",
      "25 57872.0673828125 -368.1126403808594 0.0023585206363350153\n",
      "Epoch [26/50], Train Loss: 368.6119, Val Loss: 368.5658\n",
      "26 57898.03723144531 -368.07220458984375 0.006233797408640385\n",
      "Epoch [27/50], Train Loss: 368.7773, Val Loss: 368.5854\n",
      "27 57867.28005981445 -368.0244140625 0.0015958019066601992\n",
      "Epoch [28/50], Train Loss: 368.5814, Val Loss: 368.3040\n",
      "28 57865.73370361328 -368.0581970214844 0.007926853373646736\n",
      "Epoch [29/50], Train Loss: 368.5716, Val Loss: 368.5850\n",
      "29 57873.21838378906 -368.0035095214844 0.002096987096592784\n",
      "Epoch [30/50], Train Loss: 368.6192, Val Loss: 368.3072\n",
      "30 57871.729064941406 -368.0268859863281 0.003354288637638092\n",
      "Epoch [31/50], Train Loss: 368.6097, Val Loss: 368.2194\n",
      "31 57862.262756347656 -367.8625183105469 0.0013375828275457025\n",
      "Epoch [32/50], Train Loss: 368.5494, Val Loss: 368.5119\n",
      "32 57873.42041015625 -368.0498962402344 0.04039089381694794\n",
      "Epoch [33/50], Train Loss: 368.6205, Val Loss: 368.7711\n",
      "33 57876.787170410156 -367.9947509765625 0.00707581639289856\n",
      "Epoch [34/50], Train Loss: 368.6420, Val Loss: 368.3531\n",
      "34 57868.99917602539 -367.9709167480469 0.005976410582661629\n",
      "Epoch [35/50], Train Loss: 368.5924, Val Loss: 368.3088\n",
      "35 57872.51583862305 -368.0827941894531 0.001484868349507451\n",
      "Epoch [36/50], Train Loss: 368.6148, Val Loss: 368.3608\n",
      "36 57864.36120605469 -368.07470703125 0.003614455694332719\n",
      "Epoch [37/50], Train Loss: 368.5628, Val Loss: 368.4230\n",
      "37 57860.2063293457 -368.01177978515625 0.002235840307548642\n",
      "Epoch [38/50], Train Loss: 368.5363, Val Loss: 368.3557\n",
      "38 57864.863372802734 -368.07037353515625 0.012321549467742443\n",
      "Epoch [39/50], Train Loss: 368.5660, Val Loss: 368.6004\n",
      "39 57869.14584350586 -368.0860290527344 0.0022062158677726984\n",
      "Epoch [40/50], Train Loss: 368.5933, Val Loss: 368.3112\n",
      "40 57872.90509033203 -367.9624328613281 0.08019237965345383\n",
      "Epoch [41/50], Train Loss: 368.6172, Val Loss: 368.3607\n",
      "41 57873.55471801758 -367.99517822265625 0.002928798785433173\n",
      "Epoch [42/50], Train Loss: 368.6214, Val Loss: 368.2986\n",
      "42 57870.182525634766 -368.05267333984375 0.0004262390430085361\n",
      "Epoch [43/50], Train Loss: 368.5999, Val Loss: 368.2962\n",
      "43 57864.3401184082 -368.04473876953125 0.0015153903514146805\n",
      "Epoch [44/50], Train Loss: 368.5627, Val Loss: 368.2456\n",
      "44 57890.09051513672 -368.0941162109375 0.0007223986322060227\n",
      "Epoch [45/50], Train Loss: 368.7267, Val Loss: 368.6171\n",
      "45 57859.24594116211 -367.9708251953125 0.00012987563968636096\n",
      "Epoch [46/50], Train Loss: 368.5302, Val Loss: 368.5870\n",
      "46 57882.010681152344 -367.9366760253906 0.1513763666152954\n",
      "Epoch [47/50], Train Loss: 368.6752, Val Loss: 369.2439\n",
      "47 57934.74334716797 -367.9933166503906 0.021651439368724823\n",
      "Epoch [48/50], Train Loss: 369.0111, Val Loss: 368.3738\n",
      "48 57866.87341308594 -367.96820068359375 0.001048682606779039\n",
      "Epoch [49/50], Train Loss: 368.5788, Val Loss: 368.4153\n",
      "49 57866.663818359375 -368.152099609375 0.00011715244181687012\n",
      "Epoch [50/50], Train Loss: 368.5775, Val Loss: 368.6030\n"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        p_x, q_z = vae(x)\n",
    "        log_likelihood = p_x.log_prob(x).sum(-1).mean()\n",
    "        kl = torch.distributions.kl_divergence(\n",
    "            q_z, \n",
    "            torch.distributions.Normal(0, 1.)\n",
    "        ).sum(-1).mean()\n",
    "        loss = -(log_likelihood - kl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(epoch, train_loss, log_likelihood.item(), kl.item())\n",
    "\n",
    "    vae.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            p_x, q_z = vae(x)\n",
    "            log_likelihood = p_x.log_prob(x).sum(-1).mean()\n",
    "            kl = torch.distributions.kl_divergence(\n",
    "                q_z, \n",
    "                torch.distributions.Normal(0, 1.)\n",
    "            ).sum(-1).mean()\n",
    "            loss = -(log_likelihood - kl)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
